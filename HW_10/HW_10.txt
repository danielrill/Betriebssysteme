3. One cool thing about multi.py is that you can see more detail
about what is going on with different tracing flags. Run the same
simulation as above, but this time with time left tracing enabled
(-T). This flag shows both the job that was scheduled on a CPU
at each time step, as well as how much run-time that job has left
after each tick has run.

What do you notice about how that second column decreases?


If the cache is warm the jobs are done (much) faster.
(spatial , temporal -locality)

##########################################

4. Now add one more bit of tracing, to show the status of each CPU
cache for each job, with the -C flag. For each job, each cache will
either show a blank space (if the cache is cold for that job) or a ’w’
(if the cache is warm for that job). At what point does the cache
become warm for job ’a’ in this simple example? What happens
as you change the warmup time parameter (-w) to lower or higher
values than the default?


with a higher warmup time, the jobs are slower completed
        with lower, the jobs are faster completed (warm cycle)
        
##########################################  

5. At this point, you should have a good idea of how the simula-
tor works for a single job running on a single CPU. But hey, isn’t
this a multi-processor CPU scheduling chapter? Oh yeah! So let’s
start working with multiple jobs. Specifically, let’s run the follow-
ing three jobs on a two-CPU system (i.e., type ./multi.py -n
2 -L a:100:100,b:100:50,c:100:50) Can you predict how
long this will take, given a round-robin centralized scheduler? Use
-c to see if you were right, and then dive down into details with -t

to see a step-by-step and then -C to see whether caches got warmed
effectively for these jobs. What do you notice?



CPU 0   CPU 1
   a        b               a90         b 90 |
   c        a               c90         a80   |     *6          (30sec * 6)      ~ 180sec / 2 cpu = 90 
   b        c               b80         c80  |

##########################################

6. Now we’ll apply some explicit controls to study cache affinity, as
described in the chapter. To do this, you’ll need the -A flag. This
flag can be used to limit which CPUs the scheduler can place a par-
ticular job upon. In this case, let’s use it to place jobs ’b’ and ’c’ on
CPU 1, while restricting ’a’ to CPU 0. This magic is accomplished
by typing this ./multi.py -n 2 -L a:100:100,b:100:50,
c:100:50 -A a:0,b:1,c:1 ; don’t forget to turn on various trac-
ing options to see what is really happening! Can you predict how
fast this version will run? Why does it do better? Will other com-
binations of ’a’, ’b’, and ’c’ onto the two processors run faster or
slower?


-A a:0,b:1,c:1


CPU0    CPU1

    a           b       |       a * 10                                      = 100  considering warmup  ( /2)  = 50
    a           c       |           b and c = 10 * 2 = 20 * 10  = 200   warmup after 20sec (all done)
    a           b                                                               -> so 180 / 2 = 90
    a           c                                   
                                        all in all = we considered a time max of 200 (no warmup)
                                          if we take in account that b,c are faster done (with warmup)
                                                we get the correct result of  200-90    = 110 sec
                                                
                                                
  Finished time 110

Per-CPU stats
  CPU 0  utilization 50.00 [ warm 40.91 ]
  CPU 1  utilization 100.00 [ warm 81.82 ]
  
  
  Will other combinations of ’a’, ’b’, and ’c’ onto the two processors run faster or
slower?

we will never be faster, but slower if we utilize only one cpu

##########################################

7. One interesting aspect of caching multiprocessors is the opportu-
nity for better-than-expected speed up of jobs when using multi-
ple CPUs (and their caches) as compared to running jobs on a sin-
gle processor. Specifically, when you run on N CPUs, sometimes
you can speed up by more than a factor of N, a situation entitled
super-linear speedup. To experiment with this, use the job descrip-
tion here (-L a:100:100,b:100:100,c:100:100) with a small
cache (-M 50) to create three jobs. Run this on systems with 1, 2,
and 3 CPUs (-n 1, -n 2, -n 3). Now, do the same, but with a
larger per-CPU cache of size 100. What do you notice about per-
formance as the number of CPUs scales? Use -c to confirm your
guesses, and other tracing flags to dive even deeper.


-n 1
                                          w       w        
    a[10]   b[10]   c[10]   a[10]   b[10]   c[10]       
    
    
    warmup time  = 10       cache[50]
    
CPU 0                       cache = [ a , b, c, a , b]      -> cache[ b, c , a ,b, c ]  FIFO

    a
    b
    c       [90]                                30*10  = 300 (complete) (no warmup)
    a       
    b                   cache[50] full
    c       [80]
    
    
    -> False ! cache never gets used !?
  Finished time 300

Per-CPU stats
  CPU 0  utilization 100.00 [ warm 0.00 ]  
    
    
-n2
        Finished time 150                           300 / 2  

Per-CPU stats
  CPU 0  utilization 93.75 [ warm 0.00 ]
  CPU 1  utilization 93.75 [ warm 0.00 ]
  
-n3
Finished time 100

Per-CPU stats
  CPU 0  utilization 100.00 [ warm 0.00 ]
  CPU 1  utilization 100.00 [ warm 0.00 ]
  CPU 2  utilization 100.00 [ warm 0.00 ]

###############################
cache size 100

n-1
Cant be utilized because at the moment we warm up, the job gets switched

Finished time 300

Per-CPU stats
  CPU 0  utilization 100.00 [ warm 0.00 ]


-n2

        Finished time 150                           300 / 2  

Per-CPU stats
  CPU 0  utilization 93.75 [ warm 0.00 ]
  CPU 1  utilization 93.75 [ warm 0.00 ]
  
-n3     // Considering each job runs on its own cpu we get a good result
        
Finished time 55

Per-CPU stats
  CPU 0  utilization 100.00 [ warm 81.82 ]
  CPU 1  utilization 100.00 [ warm 81.82 ]
  CPU 2  utilization 100.00 [ warm 81.82 ]
  
  
-> SpeedUp can only be utilized when the cache size is big enough( add Formula here )
    InOrder
       1 CPU = MAXLength
       2 CPU = MAXLength
       3 CPU = MAXLength / 3
############################################

8. One other aspect of the simulator worth studying is the per-CPU
scheduling option, the -p flag. Run with two CPUs again, and this
three job configuration (-L a:100:100,b:100:50,c:100:50).
How does this option do, as opposed to the hand-controlled affinity
limits you put in place above? How does performance change as
you alter the ’peek interval’ (-P) to lower or higher values? How
does this per-CPU approach work as the number of CPUs scales?


-n 2 -L a:100:100,b:100:50,c:100:50 -c -t -T -C -S -p -P 40
Finished time 115

Per-CPU stats
  CPU 0  utilization 95.65 [ warm 26.09 ]
  CPU 1  utilization 78.26 [ warm 60.87 ]


-n 2 -L a:100:100,b:100:50,c:100:50 -c -t -T -C -S -p -P 10

Finished time 100

Per-CPU stats
  CPU 0  utilization 95.00 [ warm 35.00 ]
  CPU 1  utilization 95.00 [ warm 75.00 ]


-n 2 -L a:100:100,b:100:50,c:100:50 -c -t -T -C -S -p 

Finished time 100

Per-CPU stats
  CPU 0  utilization 95.00 [ warm 35.00 ]
  CPU 1  utilization 95.00 [ warm 75.00 ]
  
  

-> Max -P each cpu will idle
    A smaller -p (peek) will cause a longer work time (cache gets full)
    a bigger peek intervall will let the workload finish faster (using cache-affinity)
    
    Overall the peek would allow us to mitigate the load imbalance between CPUS
    
############################################    
    
9. Finally, feel free to just generate random workloads and see if you
can predict their performance on different numbers of processors,
cache sizes, and scheduling options. If you do this, you’ll soon be
a multi-processor scheduling master, which is a pretty awesome
thing to be. Good luck!

 multi.py -n 4 -L a:20:40,b:10:30,c:40:40,d:10:60 -c -t -T -C -S  
 
 
 0  1   2   3               cacheSize = 100
 __________ 
 
 a  b   c   d                b = 10 ; d = 10

 a   c                         a = 10 + (10/2)  = 15   

 c                              c = 10  + 10  + 10 +(10/2)  = 35
 
 c   
 
 Finished time 35

Per-CPU stats
  CPU 0  utilization 85.71 [ warm 28.57 ]
  
  
 Considering we dont switch cpus (like c)   
 
 a  b   c   d                       c = 10 + (30/2)     = 25
 a       c
          c
          c
          
Finished time 25

Per-CPU stats
  CPU 0  utilization 60.00 [ warm 20.00 ]
  CPU 1  utilization 40.00 [ warm 0.00 ]
  CPU 2  utilization 100.00 [ warm 60.00 ]
  CPU 3  utilization 40.00 [ warm 0.00 ]

